{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c6b83a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/05/06 07:11:14 WARN Utils: Your hostname, MyPC resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/05/06 07:11:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/06 07:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# PySpark sessions for CPU and GPU\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# for CPU\n",
    "\"\"\"\n",
    "spark = SparkSession.builder \\\n",
    "\t.master(\"local\").appName(\"hdfs_test\")\\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9009\") \\\n",
    "    .getOrCreate()\n",
    "\"\"\"\n",
    "\n",
    "# for GPU\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"hdfs_test\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.task.cpus\", \"4\") \\\n",
    "    .config(\"spark.rapids.sql.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "    .config(\"spark.rapids.memory.gpu.pool.size\", \"2G\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40abb2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 07:11:16 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/05/06 07:11:16 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/05/06 07:11:16 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------+------------------+--------------------+-----+--------+------+------------+------------+-----------+--------+------------+----------------+--------------------+--------+\n",
      "|              artist|               song|emotion|          variance|               Genre|Tempo|Loudness|Energy|Danceability|Positiveness|Speechiness|Liveness|Acousticness|Instrumentalness|     embedded_vector|faiss_id|\n",
      "+--------------------+-------------------+-------+------------------+--------------------+-----+--------+------+------------+------------+-----------+--------+------------+----------------+--------------------+--------+\n",
      "|  (Alla Pugacheva...|Позови меня P.I.M.P|  anger|0.8335142456515902|Unknown,Unknown,U...|  175|   -7.15|    91|          42|          32|          8|      38|           0|              84|[-0.023154184, 0....|       0|\n",
      "|         (Ani Lorak)|         Shady Lady|    joy|0.8335142456515902|Unknown,Unknown,U...|  128|   -13.0|    70|          71|          77|          5|       7|           5|               0|[-0.05586293, 0.0...|       1|\n",
      "|        (Dima Bilan)|            Believe|    joy|0.8335142456515902|Unknown,Unknown,U...|  134|   -6.72|    73|          55|          24|          5|      22|           5|               0|[-0.031063985, 0....|       2|\n",
      "+--------------------+-------------------+-------+------------------+--------------------+-----+--------+------+------------+------------+-----------+--------+------------+----------------+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "silver_layer_path = \"file:///home/tim/al\"\n",
    "songs_embedded = spark.read.parquet(f\"{silver_layer_path}/song_info_embedded\").orderBy('faiss_id')\n",
    "songs_embedded.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e1166a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# CONVERT TO LIST OF VECTORS TO STORE IN FAISS\n",
    "\n",
    "EMD_COL_NAME = \"embedded_vector\"\n",
    "\n",
    "embedding_rows = songs_embedded.select(EMD_COL_NAME).collect()\n",
    "embedding_vectors = [row[EMD_COL_NAME] for row in embedding_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3f6645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 236600 vectors.\n"
     ]
    }
   ],
   "source": [
    "# STORE IN FAISS\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def normalize(vectors):\n",
    "    return vectors / np.linalg.norm(vectors, keepdims=True)\n",
    "\n",
    "embedding_matrix = np.array(embedding_vectors).astype(\"float32\")\n",
    "\n",
    "normalized_vectors = normalize(embedding_matrix)\n",
    "\n",
    "index = faiss.IndexFlatIP(normalized_vectors.shape[1])\n",
    "index.add(normalized_vectors)\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb21a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to 'faiss_song_index.index'\n"
     ]
    }
   ],
   "source": [
    "faiss.write_index(index, \"faiss_song_index.index\")\n",
    "print(\"FAISS index saved to 'faiss_song_index.index'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61018e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+--------------------+----------+\n",
      "| book_id|               title|      isbn|         description|     genre|\n",
      "+--------+--------------------+----------+--------------------+----------+\n",
      "|10000063|The Two Deaths of...|052595211X|Marcus Sakey retu...|   fiction|\n",
      "|10000063|The Two Deaths of...|052595211X|Marcus Sakey retu...|   mystery|\n",
      "|10000063|The Two Deaths of...|052595211X|Marcus Sakey retu...|  thriller|\n",
      "|10000063|The Two Deaths of...|052595211X|Marcus Sakey retu...|     crime|\n",
      "|10000294|     The Hoopicopter|0316903922|                    |  children|\n",
      "|10000597| Crooked Little Vein|1400175615|Michael McGill is...|    comics|\n",
      "|10000597| Crooked Little Vein|1400175615|Michael McGill is...|   graphic|\n",
      "|10000597| Crooked Little Vein|1400175615|Michael McGill is...|   fantasy|\n",
      "|10000597| Crooked Little Vein|1400175615|Michael McGill is...|paranormal|\n",
      "|10000597| Crooked Little Vein|1400175615|Michael McGill is...|   fiction|\n",
      "+--------+--------------------+----------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df = spark.read.parquet(f\"{silver_layer_path}/book_total_info\")\n",
    "books_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5d81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tim/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "# CALCULATE EMBEDDINGS FOR FEW COLUMNS WITH WEIGHTEM SUM COMBINATION\n",
    "@pandas_udf(\"array<float>\")\n",
    "def get_weighted_embeddings_udf(title: pd.Series, genre: pd.Series, description: pd.Series) -> pd.Series:\n",
    "    title_embeddings = model.encode(title.fillna(\"\").tolist(), batch_size=64)\n",
    "    genre_embeddings = model.encode(genre.fillna(\"\").tolist(), batch_size=64)\n",
    "    description_embeddings = model.encode(description.fillna(\"\").tolist(), batch_size=64)\n",
    "    \n",
    "    # Define weights\n",
    "    w_title = 0.4\n",
    "    w_genre = 0.3\n",
    "    w_description = 0.3\n",
    "\n",
    "    # Weighted sum\n",
    "    combined = [\n",
    "        (w_title * t + w_genre * g + w_description * e).tolist()\n",
    "        for t, g, e in zip(title_embeddings, genre_embeddings, description_embeddings)\n",
    "    ]\n",
    "    return pd.Series(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59262270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------+------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|book_id |genre                         |title                         |description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+--------+------------------------------+------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|10000063|crime mystery fiction thriller|The Two Deaths of Daniel Hayes|Marcus Sakey returns with his most ambitious novel, a captivating story of love and memory, where the only thing more frightening than the questions are the answers.\\nA man wakes up naked and cold, half-drowned on an abandoned beach. The only sign of life for miles is an empty BMW. Inside the expensive car he finds clothes that fit perfectly, shoes for his tattered feet, a Rolex, and a bank envelope stuffed with cash and an auto registration in the name of Daniel Hayes, resident of Malibu, California.\\nNone of it is familiar.\\nWhat is he doing here? How did he get into the ocean? Is he Daniel Hayes, and if so, why doesn't he remember? While he searches for answers, the world searches for him-beginning with the police that kick in the door of his dingy motel, with guns drawn. Lost, alone, and on the run, the man who might be Daniel Hayes flees into the night.\\nAll he remembers is a woman's face, so he sets off for the only place he might find her. The fantasy of her becomes his home, his world, his hope. And maybe, just maybe, the way back to himself.\\nBut that raises the most chilling question of all: What will he find when he gets there?|\n",
      "+--------+------------------------------+------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, concat_ws, first\n",
    "\n",
    "BOOK_ID = '10000063'\n",
    "\n",
    "# Filter all rows for the given book_id\n",
    "book = books_df.filter(books_df['book_id'] == BOOK_ID)\n",
    "\n",
    "merged_book = book.groupBy(\"book_id\") \\\n",
    "    .agg(\n",
    "        concat_ws(\" \", collect_set(\"genre\")).alias(\"genre\"),\n",
    "        first(\"title\").alias(\"title\"),\n",
    "        first(\"description\").alias(\"description\"),\n",
    "    )\n",
    "\n",
    "merged_book.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa131cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 nearest neighbors ids: [[ 15189 115843 207662 236395 190824]]\n",
      "+-----------------+-----------------+-------+-------------------+--------------------+-----+--------+------+------------+------------+-----------+--------+------------+----------------+--------------------+--------+\n",
      "|           artist|             song|emotion|           variance|               Genre|Tempo|Loudness|Energy|Danceability|Positiveness|Speechiness|Liveness|Acousticness|Instrumentalness|     embedded_vector|faiss_id|\n",
      "+-----------------+-----------------+-------+-------------------+--------------------+-----+--------+------+------------+------------+-----------+--------+------------+----------------+--------------------+--------+\n",
      "|Avenged Sevenfold|          Fiction|   fear| 0.8335142456515902|               metal|  120|   -7.45|    63|          60|          24|          3|      11|           6|               0|[-0.013874476, 0....|   15189|\n",
      "|             Kygo|          Fiction|sadness| 0.8335142456515902|          electronic|  110|   -5.52|    80|          72|          43|          3|      12|           2|               0|[-0.024039205, 0....|  115843|\n",
      "|       Status Quo|Dead In The Water|sadness|0.04300411522633744|                rock|  159|   -6.37|    87|          50|          81|         10|      11|           6|               0|[-0.015281724, 0....|  190824|\n",
      "|           The xx|          Fiction|sadness| 0.8335142456515902|                 pop|  115|  -13.79|    36|          78|          64|          4|       9|          69|              41|[-0.024795527, 0....|  207662|\n",
      "|              und|          Fiction|sadness| 0.8335142456515902|Unknown,Unknown,U...|   88|   -5.41|    77|          54|          64|          3|      40|           3|               8|[-0.0254521, 0.03...|  236395|\n",
      "+-----------------+-----------------+-------+-------------------+--------------------+-----+--------+------+------------+------------+-----------+--------+------------+----------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SEARCH WITH FAISS\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "book_with_vector = book.withColumn(\"vector\", get_weighted_embeddings_udf(\"title\", \"genre\", \"description\"))\n",
    "query_vector = book_with_vector.select(\"vector\").collect()[0][\"vector\"]\n",
    "normalized_query = normalize(np.array(query_vector, dtype=np.float32).reshape(1, -1))\n",
    "\n",
    "k = 5 \n",
    "distances, indices = index.search(normalized_query.astype(np.float32), k)\n",
    "\n",
    "print(\"Top 5 nearest neighbors ids:\", indices)\n",
    "\n",
    "def select_entries_by_row_id(row_ids, df):\n",
    "    \"\"\"\n",
    "    Select rows from the dataframe based on the provided list of row_ids.\n",
    "\n",
    "    Parameters:\n",
    "    - row_ids: A list of row_ids to filter by.\n",
    "    - df: DataFrame to select from.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame containing the rows that match the row_ids.\n",
    "    \"\"\"\n",
    "    row_id_set = set(row_ids)\n",
    "    selected_entries = df.filter(col('faiss_id').isin(row_id_set))\n",
    "    \n",
    "    return selected_entries\n",
    "\n",
    "[indices] = indices\n",
    "selected_entries_df = select_entries_by_row_id(indices, songs_embedded)\n",
    "\n",
    "selected_entries_df.show(k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
